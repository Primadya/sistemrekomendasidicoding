# -*- coding: utf-8 -*-
"""sistemrekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_XhwhcHApaw8vAVoBFxFk8FUk5bNYPy3

#   Sistem Rekomendasi Makanan
- Nauval Dwi Primadya

## Deskripsi Proyek
## Deskripsi dan Latar Belakang dari Proyek Sistem Rekomendasi Makanan

Proyek ini berfokus pada pengembangan sistem rekomendasi makanan yang menggunakan metode collaborative filtering dan content-based filtering untuk memberikan saran yang relevan dan personal kepada pengguna berdasarkan preferensi makanan mereka. Dengan meningkatnya jumlah pilihan makanan yang tersedia, sering kali pengguna merasa kesulitan dalam menentukan pilihan yang sesuai dengan selera dan kebutuhan mereka.

# 1.  Melakukan Import Library yang digunakan
"""

# Import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from pathlib import Path
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import tensorflow as tf
from tensorflow.keras import layers, regularizers

"""# 2. Data Understanding

merupakan tahapan untuk memahami informasi dari sebuah dataset dan digunakan untuk menentukan kualitas dari dataset tersebut. serta mendapatkan wawasan langkah apa saja yang akan diterapkan pada dataset tersebut.

## 2.1 Data Loading
Tahap *Data Loading* bertujuan untuk memuat dataset yang akan digunakan, sehingga mempermudah pemahaman terhadap data tersebut. Dataset ini telah melalui proses *pembersihan* dan *normalisasi* oleh penyusunnya, sehingga siap digunakan dan lebih mudah diakses, bahkan oleh pemula.

<br>

**Detail Dataset**



| Jenis      | Keterangan                                                                 |
|------------|-----------------------------------------------------------------------------|
| Title      | Food Recommendation System                                                             |
| Source     | [Kaggle](https://www.kaggle.com/datasets/schemersays/food-recommendation-system)                  |
| Maintainer | [schemersays](https://www.kaggle.com/schemersays)                                   |
| License    | Unknown                  |
| Visibility | Publik                                                                      |
| Tags       | _Busines_ |
| Usability  | 4.71                                                                      |

## 2.2 Exploratory Data Analysis **(EDA)**

*Exploratory Data Analysis* adalah proses awal dalam menyelidiki data untuk memahami karakteristiknya, mengidentifikasi pola dan anomali, serta memverifikasi asumsi yang mungkin ada pada data tersebut. Metode ini umumnya memanfaatkan teknik statistik serta visualisasi grafis untuk menyajikan informasi dengan lebih jelas.
"""

#Membaca dataset
data = pd.read_csv('/media/primadya/Kerja/dicoding/terapan/submission 2/dataset/food/1662574418893344.csv')
rating = pd.read_csv('/media/primadya/Kerja/dicoding/terapan/submission 2/dataset/food/ratings.csv')

# Ubah nama kolom menjadi lowercase
data.columns = data.columns.str.lower()
rating.columns = rating.columns.str.lower()

"""Kode tersebut digunakan untuk mengubah nama kolom dalam dua DataFrame (`data` dan `rating`) menjadi huruf kecil (lowercase). Perintah `data.columns.str.lower()` dan `rating.columns.str.lower()` mengakses nama kolom di masing-masing DataFrame dan mengubah semua karakter dalam nama kolom menjadi huruf kecil menggunakan metode string `.lower()` Setelah perubahan ini, nama kolom pada kedua DataFrame menjadi konsisten dalam format huruf kecil, yang dapat membantu menghindari kesalahan saat mengakses kolom dan memastikan keterbacaan serta konsistensi dalam pengolahan data.

### Deskripsi Variabel
"""

#Menampilkan info dari dataset
data.info()

#Menampilkan isi dari variabel data
data.head(10)

"""Berikut adalah penjelasan fitur-fitur dalam tabel :

- **`food_id`**     : ID unik untuk setiap makanan, bertipe data (integer).
- **`name`**        : Nama hidangan makanan, bertipe data (string).
- **`c_type`**      : Kategori makanan (seperti "Healthy Food", "Snack", "Dessert", dll.). bertipe data (string).
- **`veg_non`**     : Menunjukkan apakah makanan tersebut vegetarian ("veg") atau non-vegetarian ("non-veg"), bertipe data (string).
- **`describe`**    : Deskripsi bahan-bahan yang digunakan dalam hidangan, bertipe data (string).
"""

#Menampilkan info dari dataset
rating.info()

#Menampilkan isi dari variabel data
rating.head(10)

"""- **`user_id`**: ID unik untuk setiap pengguna yang memberikan rating (angka desimal, bisa menunjukkan pengguna yang berbeda).
- **`food_id`**: ID unik untuk makanan yang diberi rating, yang mengacu pada makanan tertentu dalam tabel lain (angka desimal).
- **`rating`**: Nilai rating yang diberikan oleh pengguna untuk makanan tersebut, biasanya dalam skala 1-10, di mana 10 menunjukkan penilaian terbaik (angka desimal).

### Visualisasi Data
"""

#Menghitung jumlah brand
print('Jumlah Type: ',len(data.c_type.unique()))

#Menghitung jumlah cellphone masing-masing brand
genre_counts = data['c_type'].value_counts()
print(genre_counts)

#Menampilkan dalam bentuk grafik
plt.figure(figsize=(10, 5))
sns.countplot(data=data, x=data['c_type'])
plt.xticks(rotation=90)
plt.show()

"""Kode di atas digunakan untuk menganalisis kolom `c_type` dalam DataFrame `data` yang berisi kategori jenis makanan. Pertama, kode menghitung jumlah kategori unik dalam kolom tersebut dengan menggunakan `unique()` dan `len()`, lalu menampilkan hasilnya. Selanjutnya, menggunakan `value_counts()`, kode ini menghitung dan menampilkan jumlah kemunculan masing-masing kategori. Terakhir, kode ini memvisualisasikan distribusi kategori tersebut dalam bentuk grafik batang (bar plot) dengan `seaborn.countplot()`, yang memudahkan untuk melihat perbandingan jumlah masing-masing kategori, dengan label pada sumbu X diputar 90 derajat agar lebih mudah dibaca."""

#Menghitung jumlah brand
print('Jumlah Data: ',len(data.veg_non.unique()))

#Menghitung jumlah cellphone masing-masing brand
genre_counts = data['veg_non'].value_counts()
print(genre_counts)

#Menampilkan dalam bentuk grafik
plt.figure(figsize=(5, 3))
sns.countplot(data=data, x=data['veg_non'])
plt.xticks(rotation=90)
plt.show()

"""Kode ini pertama-tama menghitung jumlah kategori unik dalam kolom `veg_non`, lalu menghitung jumlah makanan yang termasuk dalam masing-masing kategori dengan menggunakan `value_counts()`. Selanjutnya, hasil distribusi jumlah kategori tersebut ditampilkan dalam bentuk grafik batang (bar plot), yang memvisualisasikan perbandingan jumlah makanan vegetarian dan non-vegetarian."""

#Menghitung rating X muncul berapa kali
rating_counts = rating['rating'].value_counts()
print("\nJumlah kemunculan per rating:\n", rating_counts.sort_index())

#Menampilkan grafik
plt.figure(figsize=(5, 3))
sns.countplot(data=rating, x=rating['rating'])
plt.show()

"""Kode ini pertama-tama menghitung dan menampilkan jumlah kemunculan setiap nilai rating dalam kolom rating menggunakan `value_counts()`, kemudian mengurutkan hasilnya dengan `.sort_index()` untuk memudahkan pembacaan. Selanjutnya, kode ini menampilkan distribusi kemunculan rating dalam bentuk grafik batang, menggunakan `sns.countplot()`, yang memungkinkan kita untuk dengan mudah melihat berapa banyak pengguna yang memberikan rating tertentu pada data.

# 3. Data Preparation

Merupakan tahapan untuk mempersiapkan data sebelum dilakukannya pemodelan machine learning
"""

# Menggabungkan dataset berdasarkan food_id
merged_data = pd.merge(rating, data, on='food_id', how='inner', suffixes=('_user', '_food'))

"""Kode ini menggabungkan dua DataFrame (`rating` dan `data`) berdasarkan kolom `food_id` dengan menggunakan **inner join**, yang hanya menyertakan baris yang memiliki `food_id` yang cocok di kedua DataFrame. Selain itu, untuk kolom yang memiliki nama yang sama (seperti `food_id`), kode ini menambahkan akhiran `'_user'` dan `'_food'` agar tidak terjadi konflik nama kolom."""

# Informasi dataset hasil gabungan
print("Hasil penggabungan dataset:")
merged_data.head(10)

print("\nInformasi dataset hasil gabungan:")
print(merged_data.info())

# Mengubah nilai kolom menjadi lowercase
merged_data['name'] = merged_data['name'].str.lower()
merged_data['c_type'] = merged_data['c_type'].str.lower()
merged_data['describe'] = merged_data['describe'].str.lower()

"""Kode ini mengubah nilai dalam kolom `name`, `c_type`, dan `describe` pada DataFrame `merged_data` menjadi huruf kecil dengan menggunakan metode `.str.lower()`. Hal ini bertujuan untuk meningkatkan konsistensi data dan memudahkan analisis atau pencarian lebih lanjut tanpa memperhatikan perbedaan huruf besar/kecil.

### Data Cleaning

**Data Cleaning** adalah tahap krusial dalam pipeline machine learning, yang bertujuan memastikan data yang digunakan dalam pelatihan model bebas dari kesalahan, inkonsistensi, atau kekurangan. Ini mencakup menangani data yang hilang, mendeteksi dan memperbaiki outliers, mengatasi duplikasi, serta memastikan bahwa format dan tipe data sesuai. Data yang bersih memungkinkan model untuk belajar dari data yang representatif, yang pada akhirnya menghasilkan model yang lebih akurat dan dapat diandalkan.
"""

#Cek missing value
merged_data.isnull().sum()

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Menghapus nilai null
merged_data = merged_data.dropna()

# Cek missing value
print("\nJumlah missing value pada dataset:")
print(merged_data.isnull().sum())

# Menghapus duplikat berdasarkan food_id
cleaned_data = merged_data.drop_duplicates('food_id')

# Membuat DataFrame untuk digunakan
foods = pd.DataFrame({
    'food_id': cleaned_data['food_id'].tolist(),
    'name': cleaned_data['name'].tolist(),
    'c_type': cleaned_data['c_type'].tolist(),
    'veg_non': cleaned_data['veg_non'].tolist(),
    'describe': cleaned_data['describe'].tolist(),
})

"""##  TF IDF Vectorizer

**TF-IDF Vectorizer** adalah alat yang digunakan untuk mengubah teks menjadi representasi numerik berdasarkan seberapa sering kata muncul dalam teks (TF) dan seberapa penting kata tersebut dibandingkan dengan seluruh koleksi teks (IDF). Pada kode yang diberikan, TF-IDF digunakan untuk menganalisis kolom **`describe`** dalam DataFrame `foods`, mengubah deskripsi makanan menjadi bentuk yang dapat diproses lebih lanjut oleh model machine learning, serta memberikan gambaran tentang kata-kata penting dalam deskripsi makanan berdasarkan konteks seluruh dataset.
"""

# TF-IDF Vectorizer untuk kolom Describe
tf = TfidfVectorizer(stop_words='english')  # Menghapus kata umum dalam bahasa Inggris
tfidf_matrix = tf.fit_transform(foods['describe'])

# Mapping array dari fitur index integer ke fitur nama
feature_names = tf.get_feature_names_out()  # Mendapatkan nama fitur (kata-kata unik)
print("Fitur unik dari kolom 'describe':")
print(feature_names, "\n")

# Menampilkan Matriks TF-IDF dalam bentuk DataFrame
tfidf_df = pd.DataFrame(
    tfidf_matrix.todense(),  # Konversi sparse matrix ke dense matrix
    columns=feature_names,  # Kolom berdasarkan kata-kata unik
    index=foods['name']  # Nama makanan sebagai indeks
)

# Menampilkan matriks TF-IDF untuk 5 baris acak
print("\nMatriks TF-IDF untuk beberapa makanan (random 5 baris):")
print(tfidf_df.sample(5), "\n")

"""## Cosine Similarity

**Cosine Similarity** pada kode tersebut digunakan untuk menghitung tingkat kesamaan antara deskripsi makanan berdasarkan representasi TF-IDF mereka. Matriks yang dihasilkan menunjukkan seberapa mirip setiap makanan satu sama lain berdasarkan deskripsi yang ada. Matriks ini sangat berguna untuk aplikasi seperti sistem rekomendasi, di mana kita bisa menyarankan makanan yang mirip dengan makanan yang telah dipilih pengguna, atau untuk analisis teks lainnya yang membutuhkan pemahaman tentang kesamaan antara dokumen.
"""

# Menghitung cosine similarity
cosine_sim = cosine_similarity(tfidf_matrix)

# Membuat DataFrame dari matriks cosine similarity
cosine_sim_df = pd.DataFrame(cosine_sim, index=foods['name'], columns=foods['name'])

print("\n=== Contoh Cosine Similarity Matrix ===")
print(cosine_sim_df.head())

# print("=== Cosine Similarity Matrix ===")
print(cosine_sim_df.head(), "\n")

# Melihat similarity matrix pada setiap genre atau nama
print(cosine_sim_df.sample(3, axis=1).sample(3, axis=0), "\n")

"""# 4. Modelling Content Based Filtering dan Collaborative Filtering

## 4.1 Modelling Content Based Filtering (CBF)
"""

def food_recommendations(input_value, similarity_data=cosine_sim_df, items=foods[['name', 'c_type', 'veg_non']], k=4):
    input_value = input_value.strip().lower()

    # Jika input_value ada sebagai nama makanan dalam similarity_data
    matching_items = [name for name in similarity_data.columns if input_value in name.lower()]

    if matching_items:
        closest_df = pd.DataFrame({'name': matching_items})
        return closest_df.merge(items, on='name').head(k)

    # Jika input_value adalah C_Type yang ada dalam dataset
    elif any(input_value in ctype.lower() for ctype in items['c_type'].unique()):
        filtered_items = items[items['c_type'].str.contains(input_value, case=False, na=False)]
        return filtered_items.head(k)

    else:
        return f"Tidak ada rekomendasi untuk '{input_value}'."

"""Fungsi **`food_recommendations`** memberikan rekomendasi makanan berdasarkan input pengguna. Jika pengguna memasukkan nama makanan, fungsi mencari makanan yang namanya mirip. Jika tidak ada, fungsi akan mencari berdasarkan jenis makanan (misalnya, "healthy food"). Jika input tidak cocok dengan nama atau jenis makanan apapun, fungsi memberikan daftar beberapa makanan atau jenis makanan yang tersedia sebagai alternatif. Hanya `k` rekomendasi teratas yang ditampilkan.

### 4.1.1 Penggunaan Content Based Filtering (CBF)
"""

print("\nSelamat datang di sistem rekomendasi makanan!")

# Inisialisasi variabel evaluasi
total_searches = 0
total_recommendations = 0
successful_recommendations = 0

# Menyimpan data pencarian pengguna
user_searches = []

while True:
    try:
        total_searches = int(input("Masukkan jumlah pencarian yang Anda inginkan: ").strip())
        if total_searches > 0:
            break
        else:
            print("Jumlah pencarian harus lebih besar dari 0. Coba lagi.")
    except ValueError:
        print("Input tidak valid. Masukkan angka yang valid.")

for _ in range(total_searches):
    input_value = input("\nMasukkan nama makanan atau jenis makanan yang Anda suka: ").strip()
    user_searches.append(input_value)  # Catat input pengguna
    result = food_recommendations(input_value, k=10)

    print(f"\n=== Hasil Rekomendasi dari '{input_value}' ===")
    if isinstance(result, pd.DataFrame) and not result.empty:
        print(result.to_string(index=False, justify='left'))
        total_recommendations += len(result)
        successful_recommendations += 1  # Hitung pencarian yang menghasilkan rekomendasi
    else:
        print(f"Maaf, tidak ada rekomendasi yang ditemukan untuk '{input_value}'.")

# Hitung metrik evaluasi
precision = successful_recommendations / total_recommendations if total_recommendations > 0 else 0
accuracy = successful_recommendations / total_searches if total_searches > 0 else 0

# Tampilkan hasil evaluasi
print("\n=== Evaluasi ===")
print("Pencarian pengguna:")
for idx, search in enumerate(user_searches, start=1):
    print(f"{idx}. {search}")
print(f"\nTotal pencarian: {total_searches}")
print(f"Total rekomendasi: {total_recommendations}")
print(f"Pencarian dengan rekomendasi: {successful_recommendations}")
print(f"Presisi: {precision:.2%}")
print(f"Akurasi: {accuracy:.2%}")
print("\nTerima kasih telah menggunakan sistem rekomendasi makanan!")

"""Program ini mengizinkan pengguna untuk melakukan pencarian beberapa kali dan mendapatkan rekomendasi makanan berdasarkan input yang diberikan. Input yang tidak valid akan terus diproses ulang hingga benar. Setelah pencarian selesai, hasilnya ditampilkan dalam format yang mudah dibaca.

## 4.2 Modelling Collaborative Filtering
"""

# Import library
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, regularizers
import matplotlib.pyplot as plt

# Membaca dataset
df = rating
df

"""### 4.2.1 Preprocessing Dataset

**Preprocessing Dataset**:
Di bagian ini, dataset `rating` dipersiapkan untuk digunakan dalam model. Data yang hilang dihapus dengan fungsi `.dropna()`, dan kolom-kolomnya diubah menjadi huruf kecil menggunakan `.str.lower()`. Selanjutnya, ID pengguna dan ID makanan dikodekan menjadi angka dengan menggunakan fungsi `map()` untuk memetakan setiap ID ke nilai numerik yang unik, yang memungkinkan model untuk memahami input dalam bentuk numerik.
"""

#Cek missing value
df.isnull().sum()

# Menghapus nilai null
df = df.dropna()

# Cek missing value
print("\nJumlah missing value pada dataset:")
df.isnull().sum()

# Encoding user_id dan food_id
user_ids = df['user_id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

food_ids = df['food_id'].unique().tolist()
food_to_food_encoded = {x: i for i, x in enumerate(food_ids)}
food_encoded_to_food = {i: x for i, x in enumerate(food_ids)}

df['user'] = df['user_id'].map(user_to_user_encoded)
df['food'] = df['food_id'].map(food_to_food_encoded)

"""### 4.2.2 Statistik Dataset

**Statistik Dataset**:
Pada bagian ini, dilakukan perhitungan dasar tentang dataset, termasuk jumlah pengguna (`num_users`), jumlah makanan (`num_food`), serta nilai rating terendah dan tertinggi (`min_rating`, `max_rating`). Statistik ini memberikan gambaran umum tentang ukuran dan rentang rating dalam dataset yang akan digunakan untuk pelatihan model.
"""

# Statistik dataset
num_users = len(user_to_user_encoded)
num_food = len(food_to_food_encoded)
df['rating'] = df['rating'].values.astype(np.float32)
min_rating, max_rating = df['rating'].min(), df['rating'].max()

print(f'Number of Users: {num_users}, Number of Foods: {num_food}, '
      f'Min Rating: {min_rating}, Max Rating: {max_rating}')

"""### 4.2.3 Split Data

**Pembagian Data**:
Di sini, dataset diacak dengan fungsi `.sample()` untuk memastikan bahwa model tidak akan terpapar pada urutan data yang spesifik. Data kemudian dibagi menjadi dua bagian: 80% untuk data latih dan 20% untuk data validasi. Selain itu, rating dinormalisasi ke dalam rentang [0, 1] agar lebih mudah diproses oleh model, dengan menggunakan rumus normalisasi `(x - min_rating) / (max_rating - min_rating)`.
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Split dataset
df = df.sample(frac=1, random_state=42)  # Mengacak dataset
x = df[['user', 'food']].values
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values  # Normalisasi rating

# Membagi data menjadi data latih (80%) dan validasi (20%)
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices], x[train_indices:], y[:train_indices], y[train_indices:]
)

"""### 4.2.4 Modelling

Model rekomendasi RecommenderNet dibangun dengan arsitektur neural network yang menggunakan embedding layers untuk pengguna dan makanan, yang memetakan ID ke dalam ruang vektor berdimensi rendah, memungkinkan model menangkap hubungan antara keduanya. Model ini juga menggunakan bias layers untuk menangkap efek bias dari pengguna dan makanan, serta aktivasi sigmoid pada output untuk memprediksi rating dalam rentang [0, 1]. Model kemudian dikompilasi dengan loss function **BinaryCrossentropy** dan optimizer **Adam** untuk mengoptimalkan bobot model. **EarlyStopping** diterapkan untuk menghentikan pelatihan jika tidak ada peningkatan dalam metrik **RMSE** pada data validasi, mencegah overfitting dan menghemat waktu pelatihan.
"""

# Membuat Model RecommenderNet
class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_food, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(
            num_users, embedding_size, embeddings_initializer="he_normal",
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.food_embedding = layers.Embedding(
            num_food, embedding_size, embeddings_initializer="he_normal",
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.food_bias = layers.Embedding(num_food, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        food_vector = self.food_embedding(inputs[:, 1])
        food_bias = self.food_bias(inputs[:, 1])
        dot_product = tf.tensordot(user_vector, food_vector, axes=2)
        x = dot_product + user_bias + food_bias
        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_food)

# Compile model
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### 4.2.5 Penerapan collaborative filtering"""

import numpy as np

# 5. Menampilkan rekomendasi makanan untuk pengguna

# Mengambil sample user secara acak
user_id = df.user_id.sample(1).iloc[0]
food_visited_by_user = df[df.user_id == user_id]

# Menampilkan rekomendasi berdasarkan rating tinggi dari user
print(f'\nShowing recommendations for user: {user_id}')
print("===========================")

# Menampilkan makanan dengan rating tinggi dari user
# Kita anggap rating lebih besar dari 4.0 dianggap rating tinggi
high_rated_food = food_visited_by_user[food_visited_by_user['rating'] > 4.0]

print("Foods with high ratings from user:")
if not high_rated_food.empty:
    for row in high_rated_food.itertuples():
        # Mengambil nama makanan dan jenis makanan berdasarkan food_id
        food_name = data[data['food_id'] == row.food_id].iloc[0]['name']
        food_type = data[data['food_id'] == row.food_id].iloc[0]['c_type']
        print(f'Food: {food_name}, Type: {food_type}, Rating: {row.rating}')
else:
    print("No high-rated food found for this user.")

# Operator bitwise (~) untuk menemukan makanan yang belum dikunjungi oleh pengguna
food_not_visited = data[~data['food_id'].isin(food_visited_by_user.food_id.values)]['food_id']
food_not_visited = list(
    set(food_not_visited)
    .intersection(set(food_to_food_encoded.keys()))
)

# Menyusun array input untuk model untuk makanan yang belum dikunjungi
food_not_visited = [[food_to_food_encoded.get(x)] for x in food_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_food_array = np.hstack(
    ([[user_encoder]] * len(food_not_visited), food_not_visited)
)

# Prediksi rating untuk setiap food yang belum dikunjungi oleh user
ratings = model.predict(user_food_array).flatten()

# Menemukan makanan dengan rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_food_ids = [
    food_encoded_to_food.get(food_not_visited[x][0]) for x in top_ratings_indices
]

# Menampilkan rekomendasi makanan yang belum dikunjungi
print('\nTop 10 recommended foods for user:')
for food_id in recommended_food_ids:
    # Mengambil nama makanan dan jenis makanan berdasarkan food_id
    food_name = data[data['food_id'] == food_id].iloc[0]['name']
    food_type = data[data['food_id'] == food_id].iloc[0]['c_type']
    print(f'Food: {food_name}, Type: {food_type}')

# Menampilkan rekomendasi selanjutnya untuk pengguna
print("\nNext recommended food for the user:")
# Mengambil 1 makanan dengan rating tertinggi setelah prediksi
next_food_id = recommended_food_ids[0]  # Mengambil ID makanan dengan rating tertinggi
next_food = data[data['food_id'] == next_food_id]

for row in next_food.itertuples():
    food_name = row.name  # Mengambil nama makanan langsung dari data
    food_type = row.c_type  # Mengambil jenis makanan langsung dari data
    print(f'Food: {food_name}, Type: {food_type}')

"""Di bagian ini, model digunakan untuk memberikan rekomendasi makanan kepada seorang pengguna yang dipilih secara acak. Makanan yang telah dikunjungi oleh pengguna tersebut disaring, dan model memprediksi rating untuk makanan yang belum dikunjungi. Rating yang diprediksi dikembalikan ke rentang asli dan makanan dengan rating tertinggi dipilih sebagai rekomendasi. Kemudian, nama dan kategori makanan ditampilkan untuk memberikan rekomendasi yang mudah dipahami."""